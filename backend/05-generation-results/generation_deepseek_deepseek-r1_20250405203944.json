{
  "query": "DeepSeek是怎么训练出来的？",
  "timestamp": "2025-04-05T20:39:44.926852",
  "provider": "deepseek",
  "model": "deepseek-r1",
  "response": "【思维过程】\n嗯，用户问的是DeepSeek是怎么训练出来的。我需要根据提供的三个来源来回答这个问题。首先看看Source 1，里面提到了DeepSeek-R1和DeepSeek-R1-Zero，这些模型是通过强化学习（RL）来提升推理能力的。还提到了不使用监督微调（SFT）作为冷启动，直接用RL，或者用少量冷启动数据。\n\n然后Source 2详细说明了方法。他们提到之前的工作依赖大量监督数据，但DeepSeek-R1-Zero直接在基础模型上应用RL，没有用任何SFT数据。而DeepSeek-R1则是从经过数千个长链思维（CoT）示例微调的检查点开始应用RL。此外，他们还提到了将推理能力蒸馏到小型密集模型中。\n\nSource 3的目录部分也提到类似的信息，比如强化学习算法、奖励建模等部分。\n\n所以综合起来，训练方法主要有两种模型：DeepSeek-R1-Zero和DeepSeek-R1。前者是直接在基础模型上用强化学习，不需要监督数据；后者则是先用少量CoT数据进行微调，然后再用强化学习。此外，可能还涉及到奖励模型的设计和算法的选择，比如强化学习的具体实现方式，比如PPO或者其他方法。另外，奖励建模部分可能涉及如何评估模型的输出，给予合适的奖励信号，从而优化模型的表现。\n\n另外，在Source 2中还提到，DeepSeek-R1在多种任务中表现优秀，比如创意写作、问答、编辑等，这可能是因为强化学习不仅提升了推理能力，还增强了多任务处理的能力。同时，长上下文理解任务的表现优于其他模型，可能是在训练过程中特别优化了这方面的能力。\n\n需要注意的是，训练过程中可能还进行了知识蒸馏，将大模型的能力迁移到小模型上，这可能是为了提升效率和部署的便利性。不过具体的蒸馏方法在提供的上下文中没有详细说明，可能需要进一步的信息，但根据现有资料可以提到这一点。\n\n总结一下，训练过程的关键点包括：强化学习的应用，分为无监督和有监督的微调阶段，奖励模型的构建，以及可能的蒸馏步骤。同时，强调不使用大量监督数据，而是通过RL来提升能力，这也是他们的主要贡献。\n\n【最终答案】\nDeepSeek模型的训练主要基于强化学习（RL）方法，分为两种版本：**DeepSeek-R1-Zero**和**DeepSeek-R1**，具体训练过程如下：\n\n### 1. **DeepSeek-R1-Zero**\n   - **无监督强化学习**：直接在基础模型上应用强化学习，**完全未使用监督微调（SFT）数据**作为冷启动。\n   - **核心方法**：\n     - **强化学习算法**：采用大规模RL优化推理能力，通过奖励模型（Reward Model）提供反馈信号，驱动模型改进。\n     - **奖励建模**：设计奖励机制以评估模型输出质量，确保模型在推理任务中逐步提升表现。\n\n### 2. **DeepSeek-R1**\n   - **结合监督与强化学习**：\n     1. **监督微调（SFT）冷启动**：先用数千个**长链思维（Chain-of-Thought, CoT）示例**对基础模型进行微调。\n     2. **强化学习优化**：在微调后的检查点基础上进一步应用RL，进一步强化推理能力。\n   - **知识蒸馏**：将DeepSeek-R1的推理能力迁移到更小规模的密集模型中，提升效率。\n\n### 3. **关键优势**\n   - **免于大量监督数据**：相比传统方法依赖海量标注数据，DeepSeek通过RL显著减少对监督数据的依赖。\n   - **多任务泛化**：在推理（如数学、代码）、创意写作、长上下文理解等任务中表现优异（如AlpacaEval 2.0胜率87.6%，长上下文任务超越DeepSeek-V3）。\n   - **灵活扩展**：支持从零训练（Zero）到混合监督（R1）的多种训练路径，适应不同场景需求。\n\n### 总结\nDeepSeek通过创新性地结合强化学习与少量监督数据，实现了推理能力的显著提升，并在复杂任务中展现出强大的泛化性能。",
  "context": [
    {
      "text": "AIME 2024\n(Pass@1)Codeforces\n(Percentile)GPQA Diamond\n(Pass@1)MATH-500\n(Pass@1)MMLU\n(Pass@1)SWE-bench Verified\n(Resolved)020406080100\nAccuracy / Percentile (%)79.896.3\n71.597.3\n90.8\n49.279.296.6\n75.796.4\n91.8\n48.972.690.6\n62.194.3\n87.4\n36.863.693.4\n60.090.0\n85.2\n41.6\n39.258.7 59.190.2\n88.5\n42.0DeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3DeepSeek-R1 ：通过强化学习激励 LLMs 中的推理能力\n深度探索人 工 智 能\nresearch@deepseek.com\n摘要\n我们推出了第一代推理模型， DeepSeek-R1-Zero 和 DeepSeek-R1 。DeepSeek-R1-Zero 是一",
      "metadata": {
        "chunk_id": "1",
        "content": "AIME 2024\n(Pass@1)Codeforces\n(Percentile)GPQA Diamond\n(Pass@1)MATH-500\n(Pass@1)MMLU\n(Pass@1)SWE-bench Verified\n(Resolved)020406080100\nAccuracy / Percentile (%)79.896.3\n71.597.3\n90.8\n49.279.296.6\n75.796.4\n91.8\n48.972.690.6\n62.194.3\n87.4\n36.863.693.4\n60.090.0\n85.2\n41.6\n39.258.7 59.190.2\n88.5\n42.0DeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3DeepSeek-R1 ：通过强化学习激励 LLMs 中的推理能力\n深度探索人 工 智 能\nresearch@deepseek.com\n摘要\n我们推出了第一代推理模型， DeepSeek-R1-Zero 和 DeepSeek-R1 。DeepSeek-R1-Zero 是一",
        "document_name": "DeepSeek-R1-技术报告中文版.pdf"
      },
      "score": 0.597802344942321
    },
    {
      "text": "• 其他方面： DeepSeek-R1 在多种任务中也表现出色，包括创意写作、通用问答、编辑、摘\n要等。它在  AlpacaEval 2.0 上实现了  87.6% 的长度控制胜率，在  Are-naHard 上实现了  92.3\n% 的胜率，展示了其智能处理非应试查询的强大能力。此外， DeepSeek-R1 在需要长上下\n文理解的任务上表现出色，在长上下文基准测试中显著优于  DeepSeek-V3 。\n2. 方法\n2.1. 概述\n先前的工作严重依赖大量监督数据来提升模型性能。在本研究中，我们证明了即使不使用监督\n微调（SFT ）作为冷启动，通过大规模强化学习（ RL ）也能显著提升推理能力。此外，加入少\n量冷启动数据可以进一步提升性能。在接下来的章节中，我们将介绍：（ 1 ）DeepSeek-R1-Zero\n，它直接将 RL 应用于基础模型，不使用任何 SFT 数据；（ 2 ）DeepSeek-R1 ，它从经过数千个长\n链思维（ CoT ）示例微调的检查点开始应用 RL ；（3 ）将DeepSeek-R1 的推理能力蒸馏到小型密\n集模型中。\n2.2. DeepSeek-R1-Ze",
      "metadata": {
        "chunk_id": "5",
        "content": "• 其他方面： DeepSeek-R1 在多种任务中也表现出色，包括创意写作、通用问答、编辑、摘\n要等。它在  AlpacaEval 2.0 上实现了  87.6% 的长度控制胜率，在  Are-naHard 上实现了  92.3\n% 的胜率，展示了其智能处理非应试查询的强大能力。此外， DeepSeek-R1 在需要长上下\n文理解的任务上表现出色，在长上下文基准测试中显著优于  DeepSeek-V3 。\n2. 方法\n2.1. 概述\n先前的工作严重依赖大量监督数据来提升模型性能。在本研究中，我们证明了即使不使用监督\n微调（SFT ）作为冷启动，通过大规模强化学习（ RL ）也能显著提升推理能力。此外，加入少\n量冷启动数据可以进一步提升性能。在接下来的章节中，我们将介绍：（ 1 ）DeepSeek-R1-Zero\n，它直接将 RL 应用于基础模型，不使用任何 SFT 数据；（ 2 ）DeepSeek-R1 ，它从经过数千个长\n链思维（ CoT ）示例微调的检查点开始应用 RL ；（3 ）将DeepSeek-R1 的推理能力蒸馏到小型密\n集模型中。\n2.2. DeepSeek-R1-Ze",
        "document_name": "DeepSeek-R1-技术报告中文版.pdf"
      },
      "score": 0.5551384837202167
    },
    {
      "text": "目录\n1 引言 3\n1.1 贡献 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4  \u00001.2 评估结果总结  . . . . . . . . . . . . . . . . \n. . . . . . . . . . . . . 4\n2 方法 5\n2.1 概述 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5  \u00002.2 DeepSeek-R1-Zero ：基础模型\n上的强化学习  . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2.1 强化学习算法  . . . . . . . . . . . . . . . . . . . . . . 5 2.2.2 奖励建模  . . . . . . . . . . . . . . . . . . . . . . \n. . . . . . . . . . 6 2.2.",
      "metadata": {
        "chunk_id": "2",
        "content": "目录\n1 引言 3\n1.1 贡献 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4  \u00001.2 评估结果总结  . . . . . . . . . . . . . . . . \n. . . . . . . . . . . . . 4\n2 方法 5\n2.1 概述 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5  \u00002.2 DeepSeek-R1-Zero ：基础模型\n上的强化学习  . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2.1 强化学习算法  . . . . . . . . . . . . . . . . . . . . . . 5 2.2.2 奖励建模  . . . . . . . . . . . . . . . . . . . . . . \n. . . . . . . . . . 6 2.2.",
        "document_name": "DeepSeek-R1-技术报告中文版.pdf"
      },
      "score": 0.5133275152860794
    }
  ]
}
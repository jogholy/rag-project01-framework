{
  "query": "DeepSeek是怎么训练出来的？",
  "timestamp": "2025-04-05T20:40:26.294379",
  "provider": "deepseek",
  "model": "deepseek-v3",
  "response": "DeepSeek-R1 系列模型（包括 DeepSeek-R1-Zero 和 DeepSeek-R1）是通过创新的强化学习（RL）方法训练的，其核心训练流程和特点如下：\n\n---\n\n### **1. 训练方法概述**\n- **强化学习（RL）为核心**：与传统依赖大量监督微调（SFT）的方法不同，DeepSeek-R1 主要利用大规模强化学习直接提升模型的推理能力，即使在没有监督数据冷启动的情况下（如 DeepSeek-R1-Zero）也能显著提升性能。\n- **可选冷启动增强**：DeepSeek-R1 进一步结合了少量监督数据（如数千个思维链（CoT）示例的微调）作为初始检查点，再应用强化学习优化。\n\n---\n\n### **2. 具体训练流程**\n#### **2.1 DeepSeek-R1-Zero**\n- **纯强化学习训练**：直接从基础模型出发，**不依赖任何监督微调数据**，仅通过强化学习算法（如PPO）优化模型。\n- **关键组件**：\n  - **奖励建模**：设计专门的奖励函数，评估生成内容的逻辑性、准确性和连贯性。\n  - **RL算法**：通过与环境（如推理任务）的交互，最大化奖励信号，逐步提升模型能力。\n\n#### **2.2 DeepSeek-R1**\n- **两阶段训练**：\n  1. **监督微调（SFT）冷启动**：先用数千个长链思维（CoT）示例微调基础模型，建立初步推理能力。\n  2. **强化学习优化**：在SFT检查点上应用RL，进一步细化推理和泛化能力。\n- **优势**：结合少量高质量监督数据，加速RL收敛并提升最终性能。\n\n#### **2.3 能力蒸馏**\n- 将 DeepSeek-R1 的推理能力蒸馏到更小规模的密集模型中，提高效率。\n\n---\n\n### **3. 技术亮点**\n- **减少对监督数据的依赖**：证明RL可在无SFT时有效提升推理能力，突破传统数据驱动的限制。\n- **长上下文优势**：在需要长文本理解的任务中（如复杂QA、摘要），性能显著优于其他模型（如DeepSeek-V3）。\n- **多任务泛化**：在创意写作、编辑、AlpacaEval 2.0（87.6%胜率）、ArenaHard（92.",
  "context": [
    {
      "text": "AIME 2024\n(Pass@1)Codeforces\n(Percentile)GPQA Diamond\n(Pass@1)MATH-500\n(Pass@1)MMLU\n(Pass@1)SWE-bench Verified\n(Resolved)020406080100\nAccuracy / Percentile (%)79.896.3\n71.597.3\n90.8\n49.279.296.6\n75.796.4\n91.8\n48.972.690.6\n62.194.3\n87.4\n36.863.693.4\n60.090.0\n85.2\n41.6\n39.258.7 59.190.2\n88.5\n42.0DeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3DeepSeek-R1 ：通过强化学习激励 LLMs 中的推理能力\n深度探索人 工 智 能\nresearch@deepseek.com\n摘要\n我们推出了第一代推理模型， DeepSeek-R1-Zero 和 DeepSeek-R1 。DeepSeek-R1-Zero 是一",
      "metadata": {
        "chunk_id": "1",
        "content": "AIME 2024\n(Pass@1)Codeforces\n(Percentile)GPQA Diamond\n(Pass@1)MATH-500\n(Pass@1)MMLU\n(Pass@1)SWE-bench Verified\n(Resolved)020406080100\nAccuracy / Percentile (%)79.896.3\n71.597.3\n90.8\n49.279.296.6\n75.796.4\n91.8\n48.972.690.6\n62.194.3\n87.4\n36.863.693.4\n60.090.0\n85.2\n41.6\n39.258.7 59.190.2\n88.5\n42.0DeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3DeepSeek-R1 ：通过强化学习激励 LLMs 中的推理能力\n深度探索人 工 智 能\nresearch@deepseek.com\n摘要\n我们推出了第一代推理模型， DeepSeek-R1-Zero 和 DeepSeek-R1 。DeepSeek-R1-Zero 是一",
        "document_name": "DeepSeek-R1-技术报告中文版.pdf"
      },
      "score": 0.597802344942321
    },
    {
      "text": "• 其他方面： DeepSeek-R1 在多种任务中也表现出色，包括创意写作、通用问答、编辑、摘\n要等。它在  AlpacaEval 2.0 上实现了  87.6% 的长度控制胜率，在  Are-naHard 上实现了  92.3\n% 的胜率，展示了其智能处理非应试查询的强大能力。此外， DeepSeek-R1 在需要长上下\n文理解的任务上表现出色，在长上下文基准测试中显著优于  DeepSeek-V3 。\n2. 方法\n2.1. 概述\n先前的工作严重依赖大量监督数据来提升模型性能。在本研究中，我们证明了即使不使用监督\n微调（SFT ）作为冷启动，通过大规模强化学习（ RL ）也能显著提升推理能力。此外，加入少\n量冷启动数据可以进一步提升性能。在接下来的章节中，我们将介绍：（ 1 ）DeepSeek-R1-Zero\n，它直接将 RL 应用于基础模型，不使用任何 SFT 数据；（ 2 ）DeepSeek-R1 ，它从经过数千个长\n链思维（ CoT ）示例微调的检查点开始应用 RL ；（3 ）将DeepSeek-R1 的推理能力蒸馏到小型密\n集模型中。\n2.2. DeepSeek-R1-Ze",
      "metadata": {
        "chunk_id": "5",
        "content": "• 其他方面： DeepSeek-R1 在多种任务中也表现出色，包括创意写作、通用问答、编辑、摘\n要等。它在  AlpacaEval 2.0 上实现了  87.6% 的长度控制胜率，在  Are-naHard 上实现了  92.3\n% 的胜率，展示了其智能处理非应试查询的强大能力。此外， DeepSeek-R1 在需要长上下\n文理解的任务上表现出色，在长上下文基准测试中显著优于  DeepSeek-V3 。\n2. 方法\n2.1. 概述\n先前的工作严重依赖大量监督数据来提升模型性能。在本研究中，我们证明了即使不使用监督\n微调（SFT ）作为冷启动，通过大规模强化学习（ RL ）也能显著提升推理能力。此外，加入少\n量冷启动数据可以进一步提升性能。在接下来的章节中，我们将介绍：（ 1 ）DeepSeek-R1-Zero\n，它直接将 RL 应用于基础模型，不使用任何 SFT 数据；（ 2 ）DeepSeek-R1 ，它从经过数千个长\n链思维（ CoT ）示例微调的检查点开始应用 RL ；（3 ）将DeepSeek-R1 的推理能力蒸馏到小型密\n集模型中。\n2.2. DeepSeek-R1-Ze",
        "document_name": "DeepSeek-R1-技术报告中文版.pdf"
      },
      "score": 0.5551384837202167
    },
    {
      "text": "目录\n1 引言 3\n1.1 贡献 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4  \u00001.2 评估结果总结  . . . . . . . . . . . . . . . . \n. . . . . . . . . . . . . 4\n2 方法 5\n2.1 概述 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5  \u00002.2 DeepSeek-R1-Zero ：基础模型\n上的强化学习  . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2.1 强化学习算法  . . . . . . . . . . . . . . . . . . . . . . 5 2.2.2 奖励建模  . . . . . . . . . . . . . . . . . . . . . . \n. . . . . . . . . . 6 2.2.",
      "metadata": {
        "chunk_id": "2",
        "content": "目录\n1 引言 3\n1.1 贡献 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4  \u00001.2 评估结果总结  . . . . . . . . . . . . . . . . \n. . . . . . . . . . . . . 4\n2 方法 5\n2.1 概述 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5  \u00002.2 DeepSeek-R1-Zero ：基础模型\n上的强化学习  . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2.1 强化学习算法  . . . . . . . . . . . . . . . . . . . . . . 5 2.2.2 奖励建模  . . . . . . . . . . . . . . . . . . . . . . \n. . . . . . . . . . 6 2.2.",
        "document_name": "DeepSeek-R1-技术报告中文版.pdf"
      },
      "score": 0.5133275152860794
    }
  ]
}